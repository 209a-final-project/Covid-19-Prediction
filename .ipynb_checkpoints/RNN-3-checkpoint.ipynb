{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-357eeeff-ec68-4dce-bc01-619cd2007341",
    "deepnote_cell_type": "code",
    "execution_millis": 3104,
    "execution_start": 1607096571380,
    "output_cleared": false,
    "source_hash": "a6a55d27",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.3.1-cp38-cp38-macosx_10_14_x86_64.whl (165.2 MB)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Processing /Users/Angela/Library/Caches/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73/wrapt-1.12.1-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Using cached numpy-1.18.5-cp38-cp38-macosx_10_9_x86_64.whl (15.1 MB)\n",
      "Processing /Users/Angela/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501/termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.14.0-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "Collecting wheel>=0.26\n",
      "  Using cached wheel-0.36.1-py2.py3-none-any.whl (34 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Using cached absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "Collecting six>=1.12.0\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Using cached tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Using cached h5py-2.10.0-cp38-cp38-macosx_10_9_x86_64.whl (3.0 MB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Using cached tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.34.0-cp38-cp38-macosx_10_10_x86_64.whl (3.6 MB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.25.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.23.0-py2.py3-none-any.whl (114 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Collecting setuptools>=41.0.0\n",
      "  Using cached setuptools-51.0.0-py3-none-any.whl (785 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.2-py2.py3-none-any.whl (136 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Using cached rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Installing collected packages: six, numpy, keras-preprocessing, opt-einsum, wrapt, google-pasta, termcolor, protobuf, wheel, absl-py, tensorflow-estimator, gast, h5py, astunparse, werkzeug, grpcio, idna, chardet, urllib3, certifi, requests, pyasn1, rsa, pyasn1-modules, cachetools, setuptools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, markdown, tensorboard, tensorflow\n"
     ]
    }
   ],
   "source": [
    "!pip install --ignore-installed --upgrade tensorflow\n",
    "!pip install keras \n",
    "!pip install pip install cond-rnn\n",
    "!pip3 install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-99b626ee-6ca0-4ca2-9ac4-250f0cecfe39",
    "deepnote_cell_type": "code",
    "execution_millis": 12,
    "execution_start": 1607096577845,
    "output_cleared": false,
    "source_hash": "43cf1c68",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start writing code here...\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from cond_rnn import ConditionalRNN\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from hampel import hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-96b44b80-45cb-4f02-9627-8810ad9fa9fd",
    "deepnote_cell_type": "code",
    "execution_millis": 3801,
    "execution_start": 1607091976474,
    "output_cleared": false,
    "source_hash": "f31841c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import social distance data\n",
    "\n",
    "sd_data_folder_path = 'data/distance_data/'\n",
    "sd_data = pd.read_csv(sd_data_folder_path+\"county_2020.csv\")\n",
    "sd_data['fip'] = sd_data['county_fips'].apply(lambda x: str(int(x)).zfill(5))\n",
    "\n",
    "print(sd_data.shape)\n",
    "sd_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-57fe60ac-fd12-4b4b-975f-1098512331fc",
    "deepnote_cell_type": "code",
    "execution_millis": 341,
    "execution_start": 1607091983384,
    "output_cleared": false,
    "source_hash": "fdd0fcc4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate ratio of different types of trips\n",
    "sd_data[\"trip_ratio\"] = sd_data[\"trips\"]/sd_data[\"pop\"]\n",
    "sd_data[\"short_trips\"] = sd_data[\"trips_1\"] + sd_data[\"trips_1_3\"] + sd_data[\"trips_3_5\"] + sd_data[\"trips_5_10\"]\n",
    "sd_data[\"med_trips\"] = sd_data[\"trips_10_25\"] + sd_data[\"trips_25_50\"] + sd_data[\"trips_25_50\"] \n",
    "sd_data[\"long_trips\"] = sd_data[\"trips_50_100\"] + sd_data[\"trips_100_250\"] + sd_data[\"trips_250_500\"] + sd_data[\"trips_500\"] \n",
    "\n",
    "sd_data[\"short_trip_ratio\"] = sd_data[\"short_trips\"]/sd_data[\"trips\"]\n",
    "sd_data[\"med_trip_ratio\"] = sd_data[\"med_trips\"]/sd_data[\"trips\"]\n",
    "sd_data[\"long_trip_ratio\"] = sd_data[\"long_trips\"]/sd_data[\"trips\"]\n",
    "\n",
    "sd_agg = sd_data[['date', 'fip',  'trip_ratio',\"short_trip_ratio\",\n",
    "                    \"med_trip_ratio\", \"long_trip_ratio\",\n",
    "                    \"weekday\",\"avg_stay_at_home_ratio\",]]\n",
    "\n",
    "sd_agg[\"date\"] = sd_agg[\"date\"].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\n",
    "\n",
    "print(sd_agg.shape)\n",
    "sd_agg.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-3ae73142-f394-459d-92d0-06429dd19999",
    "deepnote_cell_type": "code",
    "execution_millis": 860,
    "execution_start": 1607091985778,
    "output_cleared": false,
    "source_hash": "726eee46",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load daily confirmed cases raw data\n",
    "daily_cases = pd.read_csv('data/covid_cases/covid_confirmed_usafacts_new.csv')\n",
    "daily_cases['fip'] = daily_cases['countyFIPS'].apply(lambda x: str(x).zfill(5))\n",
    "daily_cases = daily_cases[1:]\n",
    "daily_cases.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-8c7c49d8-fb8c-4bb4-aafc-c25b000bae4e",
    "deepnote_cell_type": "code",
    "execution_millis": 4367,
    "execution_start": 1607091988991,
    "output_cleared": false,
    "source_hash": "7c76fb43",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert original data to format ['fip', 'County Name', 'State', 'date', 'daily_confirmed']\n",
    "\n",
    "# get daily confirmed per row\n",
    "tmp = daily_cases.copy()\n",
    "cols = daily_cases.columns[4:-1]# daily cases data\n",
    "tmp['daily_confirmed'] = tmp[cols].values.tolist()\n",
    "# tmp['time_list'] = [list(cols)]*tmp.shape[0]\n",
    "\n",
    "# convert list of pd.Series then stack it\n",
    "tmp = (tmp\n",
    " .set_index(['fip','County Name','State'])['daily_confirmed']\n",
    " .apply(pd.Series)\n",
    " .stack()\n",
    " .reset_index()\n",
    " .rename(columns={0:'daily_confirmed'}))\n",
    "\n",
    "\n",
    "tmp2 = daily_cases.copy()\n",
    "cols = daily_cases.columns[4:-1]\n",
    "tmp2['time_list'] = [list(cols)]*tmp2.shape[0]\n",
    "\n",
    "tmp2 = (tmp2\n",
    " .set_index(['fip','County Name','State'])['time_list']\n",
    " .apply(pd.Series)\n",
    " .stack()\n",
    " .reset_index()\n",
    " .rename(columns={0:'old_date'}))\n",
    "\n",
    "cases_daily = pd.merge(tmp,tmp2,on=list(tmp.columns[:4]))\n",
    "del tmp,tmp2,daily_cases\n",
    "\n",
    "# convert date format to be consistent with social distance data\n",
    "def date_in_sd(case_date):\n",
    "    tmp = '2020-'+case_date.split('/')[0].zfill(2)+'-'+ case_date.split('/')[1].zfill(2)\n",
    "    return tmp\n",
    "\n",
    "cases_daily['date'] = cases_daily['old_date'].apply(date_in_sd)\n",
    "cases_daily = cases_daily[['fip', 'County Name', 'State', 'date', 'daily_confirmed']]\n",
    "\n",
    "cases_daily[\"date\"] = cases_daily[\"date\"].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\n",
    "\n",
    "cases_daily = cases_daily[(cases_daily[\"fip\"] != \"00000\") & (cases_daily[\"fip\"] != \"00001\")]\n",
    "cases_daily = cases_daily[cases_daily[\"date\"] < date(2020,11,1)]\n",
    "print(cases_daily.shape)\n",
    "cases_daily.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are outliers in the time series\n",
    "plt.plot(cases_daily[cases_daily.fip == \"48355\"][\"date\"], cases_daily[cases_daily.fip == \"48355\"][\"daily_confirmed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(hampel(cases_daily[cases_daily.fip == \"48355\"].reset_index(drop = True)[\"daily_confirmed\"], window_size=7, n=6)).plot()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips = list(cases_daily.fip.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle outliers in the time series\n",
    "hampeled = pd.DataFrame()\n",
    "for fip_ in tqdm(fips):\n",
    "    df = pd.DataFrame(hampel(cases_daily[cases_daily.fip == fip_].reset_index(drop = True)[\"daily_confirmed\"], window_size=7, n=3))\n",
    "    df.columns = [\"hampeled_daily_confirmed\"]\n",
    "    df[\"date\"] = cases_daily[cases_daily.fip == fip_][\"date\"].values\n",
    "    df[\"fip\"] = fip_\n",
    "    hampeled = hampeled.append(df, ignore_index = True)\n",
    "print(hampeled.shape)\n",
    "hampeled.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_daily_hampeled = cases_daily.merge(hampeled, on = [\"fip\",\"date\"])\n",
    "cases_daily_hampeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_daily_hampeled[\"new_cases\"] = cases_daily_hampeled.groupby(by = \"fip\")[\"hampeled_daily_confirmed\"].diff()\n",
    "len(cases_daily_hampeled[cases_daily_hampeled[\"new_cases\"] < 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00009-265b7cf5-fd85-4a7b-9f18-44b8c1d172cb",
    "deepnote_cell_type": "code",
    "execution_millis": 1152,
    "execution_start": 1607104744734,
    "output_cleared": false,
    "source_hash": "e05a3c07",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge cases lag data and social distance lag data\n",
    "cases_sd = pd.merge(cases_daily_hampeled,sd_agg,on = ['fip','date'], how = \"left\" ) # how = \"outer\"\n",
    "# cases_sd = cases_sd.dropna(axis=0,how='any')\n",
    "print(cases_sd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_cases_sd = pd.DataFrame()\n",
    "numerical_features = ['trip_ratio',#\"hampeled_daily_confirmed\",'daily_confirmed', 'new_cases', \n",
    "       'short_trip_ratio', 'med_trip_ratio', 'long_trip_ratio', \n",
    "       'avg_stay_at_home_ratio']\n",
    "\n",
    "for fip in tqdm(fips):\n",
    "    imputer = KNNImputer(n_neighbors=2)\n",
    "    df = cases_sd[cases_sd[\"fip\"] == fip].copy()\n",
    "    df = df.sort_values(by=['date']).reset_index(drop = True)\n",
    "    \n",
    "    num = df[numerical_features]\n",
    "    if imputer.fit_transform(num).shape[1] != len(numerical_features):\n",
    "        continue\n",
    "    num = pd.DataFrame(imputer.fit_transform(num), columns = numerical_features)\n",
    "    \n",
    "    df = df.drop(columns = numerical_features)\n",
    "    df = pd.concat([df,num], axis = 1)\n",
    "    \n",
    "    filled_cases_sd = filled_cases_sd.append(df, ignore_index=True)\n",
    "    \n",
    "print(filled_cases_sd.shape)\n",
    "filled_cases_sd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips = filled_cases_sd.fip.unique()\n",
    "print(len(fips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_cases_sd.to_csv(\"data/filled_cases_sd.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filled_cases_sd = pd.read_csv(\"data/filled_cases_sd.csv\")\n",
    "# filled_cases_sd['fip'] = filled_cases_sd['fip'].apply(lambda x: str(int(x)).zfill(5))\n",
    "# filled_cases_sd[\"date\"] = filled_cases_sd[\"date\"].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\n",
    "# # filled_cases_sd[\"case_growth_rate\"] = filled_cases_sd.groupby(by = \"fip\")[\"daily_confirmed\"].pct_change()\n",
    "# fips = filled_cases_sd.fip.unique()\n",
    "# print(len(fips))\n",
    "# print(filled_cases_sd.shape)\n",
    "# filled_cases_sd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_date = date(2020, 2, 1)\n",
    "# target_cases_sd = filled_cases_sd[filled_cases_sd[\"date\"] > start_date]\n",
    "# target_cases_sd[target_cases_sd.isnull().any(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacial_data = pd.read_csv(\"data/spacial_data/neighborcounties.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacial_data = pd.read_csv(\"data/spacial_data/neighborcounties.csv\")\n",
    "spacial_data[\"orgfips\"] = spacial_data[\"orgfips\"].apply(lambda x: str(int(x)).zfill(5))\n",
    "spacial_data[\"adjfips\"] = spacial_data[\"adjfips\"].apply(lambda x: str(int(x)).zfill(5))\n",
    "\n",
    "print(spacial_data.shape)\n",
    "spacial_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = filled_cases_sd[\"date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacial_df = pd.DataFrame()\n",
    "\n",
    "for date_ in tqdm(dates):\n",
    "    spacial_temp = spacial_data.merge(filled_cases_sd[filled_cases_sd[\"date\"] == date_], left_on = \"adjfips\",\n",
    "                                      right_on = \"fip\", how = \"left\")\n",
    "\n",
    "    spacial_temp_group = spacial_temp.groupby(by = \"orgfips\", as_index = False).mean()[[\"orgfips\",\"daily_confirmed\",\"new_cases\"]]\n",
    "    spacial_temp_group.columns = [\"orgfips\", \"mean_neighbor_daily_confirmed\", \"mean_neightbor_new_cases\"]\n",
    "    spacial_temp_group[\"date\"] = date_\n",
    "    spacial_df = spacial_df.append(spacial_temp_group,ignore_index = True)\n",
    "    \n",
    "    \n",
    "print(spacial_df.shape)\n",
    "spacial_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal = filled_cases_sd.merge(spacial_df, left_on = [\"fip\",\"date\"], right_on = [\"orgfips\",\"date\"], how = \"inner\")\n",
    "temporal = temporal.drop(columns = ['County Name', 'State','orgfips',\"weekday\"])\n",
    "temporal = temporal.dropna()\n",
    "temporal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal.groupby(by = \"date\").count()[\"fip\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal[temporal.isnull().any(axis = 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_data = pd.read_csv(\"data/hospital_bed.csv\")\n",
    "hospital_data[\"perc_over60_population\"] = hospital_data[\"Population Aged 60+\"]/hospital_data[\"Total Population\"]\n",
    "hospital_data[\"bed_over60_population_ratio\"] = hospital_data[\"ICU Beds\"]/hospital_data[\"Population Aged 60+\"]\n",
    "hospital_data = hospital_data.drop(columns = [\"ICU Beds\",\"Population Aged 60+\",\"Percent of Population Aged 60+\", \"Residents Aged 60+ Per Each ICU Bed\"])\n",
    "\n",
    "print(hospital_data.shape)\n",
    "hospital_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import social and economic data for each country\n",
    "social_data = pd.read_csv('data/social_economics/ACSDP5Y2018.DP02_data_with_overlays_2020-10-12T093324.csv')\n",
    "eco_data =  pd.read_csv('data/social_economics/ACSDP5Y2018.DP03_data_with_overlays_2020-11-09T135613.csv')\n",
    "\n",
    "# rename columns \n",
    "social_data.columns = social_data.iloc[0,:]\n",
    "social_data = social_data[1:].copy()\n",
    "\n",
    "eco_data.columns = eco_data.iloc[0,:]\n",
    "eco_data = eco_data[1:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the data\n",
    "social_eco = pd.merge(social_data, eco_data, on=['id','Geographic Area Name'])\n",
    "print('shape of social data:',social_data.shape)\n",
    "print('shape of eco data:',eco_data.shape)\n",
    "print('shape of merged data:',social_eco.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_eco = social_eco.dropna(axis=0,how='any')\n",
    "\n",
    "# Select useful columns based EDA\n",
    "useful_cols = ['id','Geographic Area Name',\n",
    "               'Estimate!!VETERAN STATUS!!Civilian population 18 years and over',\n",
    "               'Estimate!!DISABILITY STATUS OF THE CIVILIAN NONINSTITUTIONALIZED POPULATION!!Under 18 years',\n",
    "               'Estimate!!DISABILITY STATUS OF THE CIVILIAN NONINSTITUTIONALIZED POPULATION!!65 years and over',\n",
    "               'Estimate!!HOUSEHOLDS BY TYPE!!Total households!!Family households (families)!!Married-couple family',\n",
    "               'Estimate!!HOUSEHOLDS BY TYPE!!Total households!!Nonfamily households']\n",
    "col_names = ['id', 'Geographic Area Name', \"veteran\", \"disability_under_18\", \"disability_over_65\", \"married_couple\",\"nonfamily\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_eco_clean = social_eco[useful_cols].copy()\n",
    "print(social_eco_clean.shape)\n",
    "social_eco_clean.columns = col_names\n",
    "social_eco_clean['fip']=social_eco_clean['id'].apply(lambda x:x[-5:])\n",
    "social_eco_clean = social_eco_clean[social_eco_clean[\"fip\"].isin(fips)]\n",
    "social_eco_clean = social_eco_clean.drop(columns = \"id\")\n",
    "print(social_eco_clean.shape)\n",
    "social_eco_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_county_social_eco(x):\n",
    "    county,state = x.split(\",\")\n",
    "    \n",
    "    if state == \" District of Columbia\":\n",
    "        return \"the district\"\n",
    "    if state == \" Nevada\" and county == \"Carson City\":\n",
    "        return \"carson city\"\n",
    "    if state == \" New Mexico\" and county == \"Do�a Ana County\":\n",
    "        return \"dona ana\"\n",
    "    \n",
    "    county = county.split(\" \")\n",
    "    if county[-1] == \"city\" : #or \"and\" in county\n",
    "        county = \" \".join(county)\n",
    "    else:\n",
    "        county = \" \".join(county[:-1])\n",
    "    county = county.lower()\n",
    "    return county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_eco_clean[\"State\"] = social_eco_clean['Geographic Area Name'].copy().apply(lambda x: x.split(\",\")[1][1:].lower())\n",
    "social_eco_clean[\"County\"] = social_eco_clean['Geographic Area Name'].copy().apply(lambda x: extract_county_social_eco(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_data[\"State\"] = hospital_data[\"State\"].copy().apply(lambda x: x.lower())\n",
    "hospital_data[\"County\"] = hospital_data[\"County\"].copy().apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alaska_counties_se = list(social_eco_clean[social_eco_clean[\"State\"] == \"alaska\"][\"County\"].values)\n",
    "alaska_counties_hop = list(hospital_data[hospital_data[\"State\"] == \"alaska\"][\"County\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_alaska_county(x):\n",
    "    if x in alaska_counties_se:\n",
    "        return alaska_counties_hop[alaska_counties_se.index(x)] \n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_eco_clean[\"County\"] = social_eco_clean[\"County\"].apply(lambda x: clean_alaska_county(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static = social_eco_clean.merge(hospital_data, left_on = [\"State\",\"County\"],\n",
    "                                right_on = [\"State\",\"County\"], how = \"inner\")\n",
    "static = static.drop(columns = [\"State\",\"County\",\"Geographic Area Name\"])\n",
    "print(static.shape)\n",
    "static.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_scalars = {}\n",
    "cols_to_scale = [\"veteran\", \"disability_under_18\", \"disability_over_65\", \n",
    "                 \"married_couple\",\"nonfamily\", \"Total Population\",\n",
    "                 \"perc_over60_population\",\"bed_over60_population_ratio\"]\n",
    "for col in cols_to_scale:\n",
    "    scaler = StandardScaler().fit(static[[col]])\n",
    "    static_scalars[col] = scaler\n",
    "    static[col] = scaler.transform(static[[col]])\n",
    "print(static.shape)\n",
    "static.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static[\"perc_over60_population\"].hist(bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_static = temporal.merge(static, on = \"fip\", how = \"inner\")\n",
    "temp_static.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_static.to_csv(\"data/temp_static.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_static[temp_static.isnull().any(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_new_cases = min(temp_static[temp_static[\"new_cases\"] < 0][\"new_cases\"].unique())\n",
    "min_new_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 14\n",
    "fips = list(temp_static[\"fip\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_data(data, target_col, timesteps):\n",
    "    temporal_data = []\n",
    "    for i in range(1,timesteps+1):\n",
    "        temp = data.groupby('fip')[target_col].shift(i)\n",
    "        temporal_data.append(temp)\n",
    "        \n",
    "    temporal_data = np.array(temporal_data).T\n",
    "    \n",
    "    return temporal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val_test(data, splits):\n",
    "    train = data[splits[0]: splits[1]]\n",
    "    val = data[splits[1]: splits[2]]\n",
    "    test = data[splits[2]:]\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(2020,3, 1)\n",
    "total_len = temp_static.groupby(by = \"fip\").count()[\"date\"].unique()[0]\n",
    "target_len = temp_static[temp_static[\"date\"] >= start_date ].groupby(by = \"fip\").count()[\"date\"].unique()[0]\n",
    "ignore_len = total_len - target_len\n",
    "train_cutoff = ignore_len + int(target_len * 0.7)\n",
    "val_cutoff = train_cutoff + int(target_len * 0.1)\n",
    "splits = [ignore_len, train_cutoff, val_cutoff]\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_cols = [\"new_cases\", \"trip_ratio\", \n",
    "                 \"avg_stay_at_home_ratio\", \"short_trip_ratio\",\n",
    "                \"mean_neighbor_daily_confirmed\", \"mean_neightbor_new_cases\"]\n",
    "\n",
    "n_temp_features = len(temp_cols)\n",
    "\n",
    "static_cols = [\"veteran\", \"disability_under_18\", \"disability_over_65\", \n",
    "                 \"married_couple\",\"nonfamily\", \"Total Population\",\n",
    "                 \"perc_over60_population\",\"bed_over60_population_ratio\"]\n",
    "n_static_features = len(static_cols)\n",
    "print(n_temp_features, n_static_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temporal = [[], [], []]\n",
    "X_static = [[], [], [],]\n",
    "y = [[], [], [],]\n",
    "\n",
    "    \n",
    "for fip in tqdm(fips): \n",
    "    # temporal inputs\n",
    "    selected = temp_static[temp_static[\"fip\"] == fip].copy()\n",
    "    \n",
    "    temp_lags = []\n",
    "    for col in temp_cols:\n",
    "        lagged = create_temporal_data(selected, col, timesteps)\n",
    "        temp_lags.append(lagged)\n",
    "        \n",
    "#     new_cases_lagged = create_temporal_data(selected, \"new_cases\", timesteps)\n",
    "#     trip_ratio_lagged = create_temporal_data(selected, \"trip_ratio\", timesteps)\n",
    "#     sah_lagged = create_temporal_data(selected, \"avg_stay_at_home_ratio\", timesteps)\n",
    "    \n",
    "    \n",
    "    temporal_inputs = np.concatenate(temp_lags, axis = 1).reshape(total_len, timesteps, n_temp_features)\n",
    "    temp_train, temp_val, temp_test = split_train_val_test(temporal_inputs, splits)\n",
    "    X_temporal[0].append(temp_train)\n",
    "    X_temporal[1].append(temp_val)\n",
    "    X_temporal[2].append(temp_test)\n",
    "    \n",
    "    # static inputs\n",
    "    static_inputs = selected[static_cols].values\n",
    "    static_train, static_val, static_test = split_train_val_test(static_inputs, splits)\n",
    "    X_static[0].append(static_train)\n",
    "    X_static[1].append(static_val)\n",
    "    X_static[2].append(static_test)\n",
    "    \n",
    "    # response variable\n",
    "    response = selected[[\"new_cases\"]].values\n",
    "    resp_train, resp_val, resp_test = split_train_val_test(response, splits)\n",
    "    y[0].append(resp_train)\n",
    "    y[1].append(resp_val)\n",
    "    y[2].append(resp_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    X_temporal[i] = np.asarray(X_temporal[i])\n",
    "    X_temporal[i] =  X_temporal[i].reshape(X_temporal[i].shape[0]*X_temporal[i].shape[1], \n",
    "                                                  X_temporal[i].shape[2], X_temporal[i].shape[3])\n",
    "    \n",
    "    X_static[i] = np.asarray(X_static[i])\n",
    "    X_static[i] = X_static[i].reshape(X_static[i].shape[0]*X_static[i].shape[1], \n",
    "                                                  X_static[i].shape[2])\n",
    "    \n",
    "    y[i] = np.asarray(y[i]) \n",
    "    y[i] = y[i].reshape(y[i].shape[0]*y[i].shape[1], y[i].shape[2]) \n",
    "\n",
    "print(X_temporal[0].shape, X_temporal[1].shape, X_temporal[2].shape)\n",
    "print(X_static[0].shape, X_static[1].shape, X_static[2].shape)\n",
    "print(y[0].shape, y[1].shape, y[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add one so that we could use percentage error in the loss\n",
    "for i in range(3):\n",
    "    y[i] = y[i] #+ min_new_cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_static_features = X_static[0].shape[1]\n",
    "\n",
    "input_temp = tf.keras.Input(name='in_temp',shape=(timesteps,n_temp_features)) \n",
    "\n",
    "input_static = tf.keras.Input(name='in_static', shape=[n_static_features]) \n",
    "cond = ConditionalRNN(32, cell='LSTM', return_sequences=True)([input_temp, input_static]) \n",
    "dropout_cond = tf.keras.layers.Dropout(0.3)(cond)\n",
    "\n",
    "hidden = tf.keras.layers.LSTM(16)(cond)\n",
    "dropout_hidd = tf.keras.layers.Dropout(0.3)(hidden)\n",
    "predictions = tf.keras.layers.Dense(1)(dropout_hidd)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_temp, input_static], outputs=predictions) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # customize loss function\n",
    "# def mape(y_true, y_pred):\n",
    "#     sum_diff = K.abs(y_true - y_pred)\n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.isnan(X_temporal[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=20)]\n",
    "            #ReduceLROnPlateau(monitor='val_loss', patience=4, verbose=1, factor=0.6),\n",
    "             #ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss=\"mean_squared_error\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "history = model.fit([X_temporal[0],X_static[0]], y[0], \n",
    "          epochs = 200, \n",
    "          batch_size = 64, \n",
    "          validation_data=([X_temporal[1],X_static[1]], y[1]), \n",
    "          callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temporal[2][-timesteps:][:, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(history.history[\"loss\"], label = \"train\")\n",
    "plt.semilogy(history.history[\"val_loss\"], label = \"validation\")\n",
    "plt.title(\"Loss over epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict([X_temporal[2][:50],X_static[2][:50]])\n",
    "plt.plot(np.arange(0, len(y[2][:50])), pred_y, label = \"predicted\")\n",
    "plt.plot(np.arange(0, len(y[2][:50])), y[2][:50], label = \"actual\")\n",
    "plt.title(\"Prediction on test set of county1\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(4, 1, subplot_kw=dict(polar=True))\n",
    "for i in range(10):\n",
    "    fig = plt.figure()\n",
    "    pred_y = model.predict([X_temporal[2][50*i:50*(i+1)],X_static[2][50*i:50*(i+1)]])\n",
    "    plt.plot(np.arange(0, 50), pred_y.cumsum(), label = \"predicted\")\n",
    "    plt.plot(np.arange(0, 50), y[2][50*i:50*(i+1)].cumsum(), label = \"actual\")\n",
    "    plt.title(\"Prediction on test set of a county\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00019-33680db3-57bd-4de7-a6a9-3e657e184577",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# plt.title(\"Loss for NN_model\")\n",
    "# plt.semilogy(model.history.history['loss'], label='Train', color='#FF9A98')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('MSE')\n",
    "# plt.legend(loc = \"lower right\")"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "e95794eb-fcaa-495f-8219-862b6cd9100f",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
